{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "_Mah843__WebScraping_Homework.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_e07HQKhOhV"
      },
      "source": [
        "## To submit this assignment, please solve the following problem. Then, save your notebook with the format [NetID]\\_WebScraping_Homework and submit via NYU Classes. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9CvO_vHBhOhd"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQwLQUEfhOhn"
      },
      "source": [
        "### For this assignment, you must: \n",
        "\n",
        "1. Read in the WIRED Magazine's 'Top Stories' RSS Feed (https://www.wired.com/feed/rss) \n",
        "2. Create a Pandas DataFrame that, for each article, details the title of that article, its summary, and publishing date. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jh23VoCxhOh2"
      },
      "source": [
        "# import your libraries\n",
        "import time\n",
        "import re\n",
        "import csv\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "!pip install lxml #https://lxml.de/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwoHQSZQkOhH"
      },
      "source": [
        "!pip install feedparser\n",
        "import feedparser\n",
        "import random\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJhIhNmhhOiD"
      },
      "source": [
        "# read in your rss feed and parse it using feedparser\n",
        "wired_rss_url = 'https://www.wired.com/feed/rss' # find the desired rss feed\n",
        "wired_feed = feedparser.parse(wired_rss_url) # use feedparser to, well, parse the feed"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRGdM1GiknNK"
      },
      "source": [
        "print(wired_feed)\n",
        "print(len(wired_feed))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1H5rI5gvzo0"
      },
      "source": [
        "for i in range(0,int(len(wired_feed))):\n",
        "  print(wired_feed['entries'][i]['published'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zM8S8fJ9hOiP"
      },
      "source": [
        "# navigate (using bracket notation) to find the information about each article"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnrTrjUthOiX"
      },
      "source": [
        "# hint\n",
        "\n",
        "title_set = set()\n",
        "summary_set = set()\n",
        "published_set = list() #If I put this to a set, it only considers the articles published at a unique time. So, I changed it to list.\n",
        "\n",
        "# find your title, summary, and published date from the feed \n",
        "for i in range(0,int(len(wired_feed))):\n",
        "  title_set.add(wired_feed['entries'][i]['title']) #details the title of that article\n",
        "  summary_set.add(wired_feed['entries'][i]['summary']) #Summary of the article \n",
        "  published_set.append(wired_feed['entries'][i]['published']) #Publishing date of the article\n",
        "    # add that information to the respective sets"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-RP_77N-lsgg"
      },
      "source": [
        "title_set #A set that details of each title of that article"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICAJtadZltIC"
      },
      "source": [
        "summary_set #A set that details of each summary of that article"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vS_vEKeAltRh"
      },
      "source": [
        "published_set  # A set that details of each publication of that article"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "av3yKlvOhOij"
      },
      "source": [
        "# hint 2\n",
        "import pandas as pd\n",
        "\n",
        "data_tuples = list(zip(title_set,summary_set,published_set))\n",
        "\n",
        "df_wired = pd.DataFrame(data_tuples, columns=['Title','Summary','Publishing Date'])"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CaM9Iox-pkFM"
      },
      "source": [
        "df_wired"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}